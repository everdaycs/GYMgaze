#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å…¨å±€åœ°å›¾é¢„æµ‹æ¨¡å‹è®­ç»ƒè„šæœ¬

ä»»åŠ¡ï¼šä»å±€éƒ¨è§‚æµ‹+å†å²ç´¯ç§¯ä¿¡æ¯ â†’ é¢„æµ‹å®Œæ•´å…¨å±€åœ°å›¾
ç±»ä¼¼SLAMä¸­çš„åœ°å›¾æ„å»ºï¼Œä½†ä½¿ç”¨æ·±åº¦å­¦ä¹ è¿›è¡Œé¢„æµ‹
"""

import os
import sys
import pickle
import argparse
import numpy as np
import matplotlib.pyplot as plt
from tqdm import tqdm

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
_PROJECT_ROOT = os.path.dirname(os.path.abspath(__file__))
if _PROJECT_ROOT not in sys.path:
    sys.path.insert(0, _PROJECT_ROOT)

# ä» src/models å¯¼å…¥æ¨¡å‹å®šä¹‰
from src.models.global_map import GlobalMapPredictor, ConvBlock


# ============== æ•°æ®é›† ==============

class GlobalMapDataset(Dataset):
    """å…¨å±€åœ°å›¾é¢„æµ‹æ•°æ®é›†"""
    
    def __init__(self, data_path: str, sequence_length: int = 5):
        self.sequence_length = sequence_length
        self.samples = []
        
        # åŠ è½½æ•°æ®
        with open(data_path, 'rb') as f:
            episodes = pickle.load(f)
        
        # å±•å¼€æ‰€æœ‰åºåˆ—
        for ep in episodes:
            for seq in ep['sequences']:
                self.samples.append(seq)
        
        print(f"åŠ è½½æ•°æ®é›†: {len(self.samples)} ä¸ªæ ·æœ¬")
    
    def __len__(self):
        return len(self.samples)
    
    def __getitem__(self, idx):
        sample = self.samples[idx]
        
        # æ„å»ºè¾“å…¥å¼ é‡
        # è¾“å…¥é€šé“ï¼š
        # 1. åºåˆ—å¸§çš„local_occupancy (Tå¸§)
        # 2. å½“å‰çš„global_accumulated
        # 3. global_visit_count (å½’ä¸€åŒ–)
        
        frames = sample['sequence_frames']
        T = len(frames)
        
        # è·å–æœ€åä¸€å¸§çš„å…¨å±€ç´¯ç§¯ä¿¡æ¯
        last_frame = frames[-1]
        global_acc = last_frame['global_accumulated'].astype(np.float32) / 255.0
        global_visit = np.clip(last_frame['global_visit_count'].astype(np.float32) / 100.0, 0, 1)
        
        # æ„å»ºæ—¶é—´åºåˆ—è¾“å…¥ (T, H, W)
        local_seq = np.stack([
            f['local_occupancy'].astype(np.float32) / 255.0 
            for f in frames
        ], axis=0)
        
        # åˆ›å»ºå·²çŸ¥åŒºåŸŸæ©ç  (H, W)
        known_mask = (last_frame['global_accumulated'] != 127).astype(np.float32)
        
        # ç»„åˆè¾“å…¥ (T+3, H, W)
        # - Tå¸§å±€éƒ¨è§‚æµ‹
        # - 1å¸§å…¨å±€ç´¯ç§¯
        # - 1å¸§è®¿é—®è®¡æ•°
        # - 1å¸§å·²çŸ¥æ©ç 
        input_tensor = np.concatenate([
            local_seq,                          # (T, H, W)
            global_acc[np.newaxis, :, :],       # (1, H, W)
            global_visit[np.newaxis, :, :],     # (1, H, W)
            known_mask[np.newaxis, :, :]        # (1, H, W)
        ], axis=0)
        
        # Ground Truth: å®Œæ•´å…¨å±€åœ°å›¾
        gt = sample['global_ground_truth'].astype(np.float32)
        # å°†-1(è¾¹ç•Œ)è½¬ä¸º0.5ï¼Œ0(ç©ºé—²)ä¿æŒ0ï¼Œ1(éšœç¢ç‰©)ä¿æŒ1
        gt_tensor = np.where(gt == -1, 0.5, gt)
        
        # æœ‰æ•ˆåŒºåŸŸæ©ç ï¼ˆæ’é™¤è¾¹ç•Œï¼‰
        valid_mask = (sample['global_ground_truth'] >= 0).astype(np.float32)
        
        return (
            torch.from_numpy(input_tensor),
            torch.from_numpy(gt_tensor),
            torch.from_numpy(valid_mask),
            torch.from_numpy(known_mask)
        )


# ============== è®­ç»ƒå™¨ ==============

class GlobalMapTrainer:
    """å…¨å±€åœ°å›¾é¢„æµ‹è®­ç»ƒå™¨"""
    
    def __init__(self, model, device='cuda'):
        self.model = model.to(device)
        self.device = device
        
        # æŸå¤±å‡½æ•°ï¼šç»“åˆBCEå’Œå·²çŸ¥åŒºåŸŸçº¦æŸ
        self.bce = nn.BCELoss(reduction='none')
        
        # å·²çŸ¥åŒºåŸŸæƒé‡æ›´é«˜ï¼ˆç¡®ä¿å·²çŸ¥åŒºåŸŸé¢„æµ‹å‡†ç¡®ï¼‰
        self.known_weight = 2.0
        # æœªçŸ¥åŒºåŸŸä¸­éšœç¢ç‰©æƒé‡
        self.unknown_obs_weight = 5.0
    
    def compute_loss(self, pred, target, valid_mask, known_mask):
        """
        è®¡ç®—æŸå¤±
        
        pred: (B, H, W) é¢„æµ‹
        target: (B, H, W) çœŸå®æ ‡ç­¾
        valid_mask: (B, H, W) æœ‰æ•ˆåŒºåŸŸï¼ˆæ’é™¤è¾¹ç•Œï¼‰
        known_mask: (B, H, W) å·²çŸ¥åŒºåŸŸ
        """
        # åŸºç¡€BCEæŸå¤±
        bce_loss = self.bce(pred, target)
        
        # æƒé‡çŸ©é˜µ
        weight = torch.ones_like(pred)
        
        # å·²çŸ¥åŒºåŸŸæƒé‡æ›´é«˜
        weight = weight + known_mask * (self.known_weight - 1)
        
        # æœªçŸ¥åŒºåŸŸä¸­çš„éšœç¢ç‰©æƒé‡æ›´é«˜
        unknown_mask = (1 - known_mask) * valid_mask
        unknown_obstacle = unknown_mask * target
        weight = weight + unknown_obstacle * (self.unknown_obs_weight - 1)
        
        # åŠ æƒæŸå¤±
        weighted_loss = bce_loss * weight * valid_mask
        
        # å¹³å‡
        loss = weighted_loss.sum() / (valid_mask.sum() + 1e-8)
        
        return loss
    
    def train_epoch(self, dataloader, optimizer):
        self.model.train()
        total_loss = 0
        
        for inputs, targets, valid_masks, known_masks in tqdm(dataloader, desc="Training"):
            inputs = inputs.to(self.device)
            targets = targets.to(self.device)
            valid_masks = valid_masks.to(self.device)
            known_masks = known_masks.to(self.device)
            
            # Forward
            outputs = self.model(inputs, known_masks.unsqueeze(1))
            
            # Loss
            loss = self.compute_loss(outputs, targets, valid_masks, known_masks)
            
            # Backward
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            
            total_loss += loss.item()
        
        return total_loss / len(dataloader)
    
    def evaluate(self, dataloader):
        self.model.eval()
        total_loss = 0
        
        # åˆ†åˆ«ç»Ÿè®¡å·²çŸ¥åŒºåŸŸå’ŒæœªçŸ¥åŒºåŸŸçš„å‡†ç¡®ç‡
        known_correct = 0
        known_total = 0
        unknown_correct = 0
        unknown_total = 0
        
        with torch.no_grad():
            for inputs, targets, valid_masks, known_masks in dataloader:
                inputs = inputs.to(self.device)
                targets = targets.to(self.device)
                valid_masks = valid_masks.to(self.device)
                known_masks = known_masks.to(self.device)
                
                outputs = self.model(inputs, known_masks.unsqueeze(1))
                
                loss = self.compute_loss(outputs, targets, valid_masks, known_masks)
                total_loss += loss.item()
                
                # è®¡ç®—å‡†ç¡®ç‡
                pred_binary = (outputs > 0.5).float()
                correct = (pred_binary == targets) * valid_masks
                
                # å·²çŸ¥åŒºåŸŸå‡†ç¡®ç‡
                known_correct += (correct * known_masks).sum().item()
                known_total += (valid_masks * known_masks).sum().item()
                
                # æœªçŸ¥åŒºåŸŸå‡†ç¡®ç‡
                unknown_mask = (1 - known_masks) * valid_masks
                unknown_correct += (correct * unknown_mask).sum().item()
                unknown_total += unknown_mask.sum().item()
        
        known_acc = known_correct / (known_total + 1e-8)
        unknown_acc = unknown_correct / (unknown_total + 1e-8)
        
        return total_loss / len(dataloader), known_acc, unknown_acc


def main():
    parser = argparse.ArgumentParser(description='è®­ç»ƒå…¨å±€åœ°å›¾é¢„æµ‹æ¨¡å‹')
    parser.add_argument('--epochs', type=int, default=50)
    parser.add_argument('--batch-size', type=int, default=4)
    parser.add_argument('--lr', type=float, default=1e-3)
    parser.add_argument('--data-dir', type=str, default='./data/global_map_training_data')
    parser.add_argument('--model-path', type=str, default='./checkpoints/global_map_model.pth')
    parser.add_argument('--sequence-length', type=int, default=5)
    
    args = parser.parse_args()
    
    print("=" * 60)
    print("ğŸ—ºï¸  è®­ç»ƒå…¨å±€åœ°å›¾é¢„æµ‹æ¨¡å‹")
    print("=" * 60)
    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")
    
    # åŠ è½½æ•°æ®
    data_path = os.path.join(args.data_dir, 'training_data.pkl')
    dataset = GlobalMapDataset(data_path, args.sequence_length)
    
    # åˆ’åˆ†è®­ç»ƒ/éªŒè¯é›†
    train_size = int(0.8 * len(dataset))
    val_size = len(dataset) - train_size
    train_dataset, val_dataset = torch.utils.data.random_split(
        dataset, [train_size, val_size]
    )
    
    train_loader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=args.batch_size)
    
    print(f"è®­ç»ƒé›†: {train_size} æ ·æœ¬")
    print(f"éªŒè¯é›†: {val_size} æ ·æœ¬")
    
    # åˆ›å»ºæ¨¡å‹
    # è¾“å…¥é€šé“ï¼šTå¸§å±€éƒ¨è§‚æµ‹ + å…¨å±€ç´¯ç§¯ + è®¿é—®è®¡æ•° + å·²çŸ¥æ©ç 
    in_channels = args.sequence_length + 3
    model = GlobalMapPredictor(in_channels=in_channels, base_channels=32)
    
    print(f"æ¨¡å‹å‚æ•°é‡: {sum(p.numel() for p in model.parameters()):,}")
    
    # è®­ç»ƒå™¨
    trainer = GlobalMapTrainer(model, device)
    optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)
    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
        optimizer, mode='min', factor=0.5, patience=5
    )
    
    # è®­ç»ƒå¾ªç¯
    best_val_loss = float('inf')
    history = {'train_loss': [], 'val_loss': [], 'known_acc': [], 'unknown_acc': []}
    
    for epoch in range(args.epochs):
        print(f"\nğŸ“Š Epoch {epoch+1}/{args.epochs}")
        
        train_loss = trainer.train_epoch(train_loader, optimizer)
        val_loss, known_acc, unknown_acc = trainer.evaluate(val_loader)
        
        history['train_loss'].append(train_loss)
        history['val_loss'].append(val_loss)
        history['known_acc'].append(known_acc)
        history['unknown_acc'].append(unknown_acc)
        
        print(f"Train Loss: {train_loss:.4f}")
        print(f"Val Loss: {val_loss:.4f}")
        print(f"å·²çŸ¥åŒºåŸŸå‡†ç¡®ç‡: {known_acc*100:.1f}%")
        print(f"æœªçŸ¥åŒºåŸŸå‡†ç¡®ç‡: {unknown_acc*100:.1f}%")
        
        scheduler.step(val_loss)
        
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            torch.save({
                'model_state_dict': model.state_dict(),
                'epoch': epoch,
                'val_loss': val_loss
            }, args.model_path)
            print(f"ğŸ’¾ ä¿å­˜æœ€ä½³æ¨¡å‹ (Val Loss: {val_loss:.4f})")
    
    print(f"\nâœ… è®­ç»ƒå®Œæˆï¼æœ€ä½³éªŒè¯æŸå¤±: {best_val_loss:.4f}")
    
    # ä¿å­˜è®­ç»ƒæ›²çº¿
    fig, axes = plt.subplots(1, 2, figsize=(12, 4))
    
    axes[0].plot(history['train_loss'], label='Train')
    axes[0].plot(history['val_loss'], label='Val')
    axes[0].set_xlabel('Epoch')
    axes[0].set_ylabel('Loss')
    axes[0].legend()
    axes[0].set_title('Loss Curve')
    
    axes[1].plot(history['known_acc'], label='Known Region')
    axes[1].plot(history['unknown_acc'], label='Unknown Region')
    axes[1].set_xlabel('Epoch')
    axes[1].set_ylabel('Accuracy')
    axes[1].legend()
    axes[1].set_title('Accuracy by Region')
    
    plt.tight_layout()
    plt.savefig('global_map_training_history.png', dpi=150)
    print("ğŸ“ˆ è®­ç»ƒæ›²çº¿å·²ä¿å­˜")


if __name__ == "__main__":
    main()
